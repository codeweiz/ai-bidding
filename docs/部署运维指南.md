# AIæŠ•æ ‡æ–¹æ¡ˆç”Ÿæˆç³»ç»Ÿ - éƒ¨ç½²è¿ç»´æŒ‡å—

## ğŸ“‹ æ–‡æ¡£ä¿¡æ¯
- **æ–‡æ¡£ç‰ˆæœ¬**ï¼šv2.0
- **é€‚ç”¨ç¯å¢ƒ**ï¼šå¼€å‘ã€æµ‹è¯•ã€ç”Ÿäº§
- **æœ€åæ›´æ–°**ï¼š2025-07-02
- **ç»´æŠ¤å›¢é˜Ÿ**ï¼šAIæŠ•æ ‡ç³»ç»Ÿå¼€å‘å›¢é˜Ÿ

## ğŸ“‹ ç›®å½•
- [1. ç¯å¢ƒè¦æ±‚](#1-ç¯å¢ƒè¦æ±‚)
- [2. å¿«é€Ÿéƒ¨ç½²](#2-å¿«é€Ÿéƒ¨ç½²)
- [3. ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²](#3-ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²)
- [4. é…ç½®ç®¡ç†](#4-é…ç½®ç®¡ç†)
- [5. ç›‘æ§å‘Šè­¦](#5-ç›‘æ§å‘Šè­¦)
- [6. æ—¥å¿—ç®¡ç†](#6-æ—¥å¿—ç®¡ç†)
- [7. å¤‡ä»½æ¢å¤](#7-å¤‡ä»½æ¢å¤)
- [8. æ•…éšœæ’é™¤](#8-æ•…éšœæ’é™¤)

## 1. ç¯å¢ƒè¦æ±‚

### 1.1 ç¡¬ä»¶è¦æ±‚

#### å¼€å‘ç¯å¢ƒ
- **CPU**ï¼š2æ ¸å¿ƒä»¥ä¸Š
- **å†…å­˜**ï¼š4GBä»¥ä¸Š
- **å­˜å‚¨**ï¼š20GBå¯ç”¨ç©ºé—´
- **ç½‘ç»œ**ï¼šç¨³å®šçš„äº’è”ç½‘è¿æ¥

#### æµ‹è¯•ç¯å¢ƒ
- **CPU**ï¼š4æ ¸å¿ƒä»¥ä¸Š
- **å†…å­˜**ï¼š8GBä»¥ä¸Š
- **å­˜å‚¨**ï¼š50GBå¯ç”¨ç©ºé—´
- **ç½‘ç»œ**ï¼šç¨³å®šçš„äº’è”ç½‘è¿æ¥

#### ç”Ÿäº§ç¯å¢ƒ
- **CPU**ï¼š8æ ¸å¿ƒä»¥ä¸Š
- **å†…å­˜**ï¼š16GBä»¥ä¸Š
- **å­˜å‚¨**ï¼š100GBå¯ç”¨ç©ºé—´ï¼ˆSSDæ¨èï¼‰
- **ç½‘ç»œ**ï¼šé«˜é€Ÿç¨³å®šç½‘ç»œè¿æ¥
- **è´Ÿè½½å‡è¡¡**ï¼šæ”¯æŒå¤šå®ä¾‹éƒ¨ç½²

### 1.2 è½¯ä»¶è¦æ±‚

#### åŸºç¡€è½¯ä»¶
- **æ“ä½œç³»ç»Ÿ**ï¼šUbuntu 20.04+ / CentOS 8+ / macOS 10.15+
- **Docker**ï¼š20.0+
- **Docker Compose**ï¼š2.0+
- **Python**ï¼š3.11+ï¼ˆå¦‚æœæœ¬åœ°å¼€å‘ï¼‰

#### å¤–éƒ¨ä¾èµ–
- **PostgreSQL**ï¼š15+
- **Redis**ï¼š7+
- **Nginx**ï¼š1.20+ï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰

### 1.3 ç½‘ç»œè¦æ±‚
- **ç«¯å£å¼€æ”¾**ï¼š
  - 8000 (åç«¯API)
  - 7860 (å‰ç«¯ç•Œé¢)
  - 5555 (Celeryç›‘æ§)
  - 5432 (PostgreSQL)
  - 6379 (Redis)
  - 80/443 (Nginx)

## 2. å¿«é€Ÿéƒ¨ç½²

### 2.1 å¼€å‘ç¯å¢ƒéƒ¨ç½²

#### 2.1.1 å…‹éš†ä»£ç 
```bash
git clone https://github.com/your-org/ai-bidding.git
cd ai-bidding
```

#### 2.1.2 é…ç½®ç¯å¢ƒ
```bash
# å¤åˆ¶é…ç½®æ–‡ä»¶
cp config.toml.example config.toml

# ç¼–è¾‘é…ç½®æ–‡ä»¶ï¼Œè®¾ç½®APIå¯†é’¥
vim config.toml
```

#### 2.1.3 å¯åŠ¨æœåŠ¡
```bash
# ä½¿ç”¨å¢å¼ºç‰ˆå¯åŠ¨è„šæœ¬
./start_enhanced.sh

# æˆ–è€…ä½¿ç”¨Makeå‘½ä»¤
make docker-compose-up
```

#### 2.1.4 éªŒè¯éƒ¨ç½²
```bash
# æ£€æŸ¥æœåŠ¡çŠ¶æ€
curl http://localhost:8000/health

# è®¿é—®å‰ç«¯ç•Œé¢
open http://localhost:7860

# è®¿é—®APIæ–‡æ¡£
open http://localhost:8000/docs

# è®¿é—®Celeryç›‘æ§
open http://localhost:5555
```

### 2.2 ä½¿ç”¨Docker Compose

#### 2.2.1 åŸºç¡€éƒ¨ç½²
```bash
# å¯åŠ¨æ‰€æœ‰æœåŠ¡
docker-compose -f docker-compose.enhanced.yml up -d

# æŸ¥çœ‹æœåŠ¡çŠ¶æ€
docker-compose -f docker-compose.enhanced.yml ps

# æŸ¥çœ‹æ—¥å¿—
docker-compose -f docker-compose.enhanced.yml logs -f
```

#### 2.2.2 æœåŠ¡ç®¡ç†
```bash
# åœæ­¢æœåŠ¡
docker-compose -f docker-compose.enhanced.yml down

# é‡å¯ç‰¹å®šæœåŠ¡
docker-compose -f docker-compose.enhanced.yml restart backend

# æ‰©å±•Workeræ•°é‡
docker-compose -f docker-compose.enhanced.yml up -d --scale celery-worker=3
```

## 3. ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

### 3.1 ç”Ÿäº§ç¯å¢ƒæ¶æ„

```mermaid
graph TB
    subgraph "è´Ÿè½½å‡è¡¡å±‚"
        LB[Nginxè´Ÿè½½å‡è¡¡å™¨]
        SSL[SSLç»ˆç«¯]
    end
    
    subgraph "åº”ç”¨å±‚"
        API1[åç«¯API-1]
        API2[åç«¯API-2]
        API3[åç«¯API-3]
        WORKER1[Celery Worker-1]
        WORKER2[Celery Worker-2]
        WORKER3[Celery Worker-3]
        BEAT[Celery Beat]
        FLOWER[Flowerç›‘æ§]
    end
    
    subgraph "æ•°æ®å±‚"
        PG_MASTER[(PostgreSQLä¸»åº“)]
        PG_SLAVE[(PostgreSQLä»åº“)]
        REDIS_MASTER[(Redisä¸»èŠ‚ç‚¹)]
        REDIS_SLAVE[(Redisä»èŠ‚ç‚¹)]
    end
    
    subgraph "å­˜å‚¨å±‚"
        NFS[å…±äº«æ–‡ä»¶å­˜å‚¨]
        BACKUP[å¤‡ä»½å­˜å‚¨]
    end
    
    subgraph "ç›‘æ§å±‚"
        PROMETHEUS[Prometheus]
        GRAFANA[Grafana]
        ALERTMANAGER[AlertManager]
    end
    
    LB --> API1
    LB --> API2
    LB --> API3
    
    API1 --> PG_MASTER
    API2 --> PG_MASTER
    API3 --> PG_MASTER
    
    PG_MASTER --> PG_SLAVE
    REDIS_MASTER --> REDIS_SLAVE
    
    WORKER1 --> PG_MASTER
    WORKER2 --> PG_MASTER
    WORKER3 --> PG_MASTER
    
    API1 --> REDIS_MASTER
    WORKER1 --> REDIS_MASTER
    
    API1 --> NFS
    WORKER1 --> NFS
    
    PROMETHEUS --> API1
    PROMETHEUS --> WORKER1
    GRAFANA --> PROMETHEUS
    ALERTMANAGER --> PROMETHEUS
```

### 3.2 ç”Ÿäº§ç¯å¢ƒé…ç½®

#### 3.2.1 ç¯å¢ƒå˜é‡é…ç½®
```bash
# åˆ›å»ºç”Ÿäº§ç¯å¢ƒé…ç½®æ–‡ä»¶
cat > .env.prod << EOF
# æ•°æ®åº“é…ç½®
DATABASE_URL=postgresql+asyncpg://ai_bidding:secure_password@postgres-master:5432/ai_bidding
DATABASE_POOL_SIZE=20
DATABASE_MAX_OVERFLOW=30

# Redisé…ç½®
REDIS_URL=redis://redis-master:6379/0
REDIS_POOL_SIZE=10

# Celeryé…ç½®
CELERY_BROKER_URL=redis://redis-master:6379/0
CELERY_RESULT_BACKEND=redis://redis-master:6379/0
CELERY_WORKER_CONCURRENCY=4

# å®‰å…¨é…ç½®
JWT_SECRET=your-super-secret-jwt-key
ENCRYPTION_KEY=your-encryption-key

# æ—¥å¿—é…ç½®
LOG_LEVEL=INFO
LOG_FORMAT=json

# ç›‘æ§é…ç½®
PROMETHEUS_ENABLED=true
METRICS_PORT=9090

# æ–‡ä»¶å­˜å‚¨é…ç½®
UPLOAD_PATH=/shared/uploads
OUTPUT_PATH=/shared/outputs
MAX_FILE_SIZE=52428800  # 50MB

# LLMé…ç½®
LLM_PROVIDER=deepseek
LLM_API_KEY=your-llm-api-key
LLM_MODEL=deepseek-chat
LLM_TIMEOUT=300
LLM_MAX_RETRIES=3
EOF
```

#### 3.2.2 Docker Composeç”Ÿäº§é…ç½®
```yaml
# docker-compose.prod.yml
version: '3.8'

services:
  postgres-master:
    image: postgres:15-alpine
    environment:
      POSTGRES_DB: ai_bidding
      POSTGRES_USER: ai_bidding
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - postgres_master_data:/var/lib/postgresql/data
      - ./postgresql.conf:/etc/postgresql/postgresql.conf
    command: postgres -c config_file=/etc/postgresql/postgresql.conf
    networks:
      - ai-bidding-network
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2'
        reservations:
          memory: 2G
          cpus: '1'

  redis-master:
    image: redis:7-alpine
    command: redis-server --appendonly yes --maxmemory 2gb --maxmemory-policy allkeys-lru
    volumes:
      - redis_master_data:/data
    networks:
      - ai-bidding-network
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1'

  backend:
    image: ai-bidding:${VERSION}
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      - JWT_SECRET=${JWT_SECRET}
    volumes:
      - shared_storage:/shared
    networks:
      - ai-bidding-network
    deploy:
      replicas: 3
      resources:
        limits:
          memory: 2G
          cpus: '1'
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3

  celery-worker:
    image: ai-bidding:${VERSION}
    command: celery -A backend.tasks.celery_app worker --loglevel=info --concurrency=4
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
    volumes:
      - shared_storage:/shared
    networks:
      - ai-bidding-network
    deploy:
      replicas: 3
      resources:
        limits:
          memory: 4G
          cpus: '2'

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.prod.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/nginx/ssl
      - shared_storage:/var/www/shared
    networks:
      - ai-bidding-network
    depends_on:
      - backend

volumes:
  postgres_master_data:
  redis_master_data:
  shared_storage:
    driver: local
    driver_opts:
      type: nfs
      o: addr=nfs-server,rw
      device: ":/shared/ai-bidding"

networks:
  ai-bidding-network:
    driver: overlay
    attachable: true
```

### 3.3 Nginxç”Ÿäº§é…ç½®

#### 3.3.1 è´Ÿè½½å‡è¡¡é…ç½®
```nginx
# nginx.prod.conf
upstream backend_servers {
    least_conn;
    server backend_1:8000 max_fails=3 fail_timeout=30s;
    server backend_2:8000 max_fails=3 fail_timeout=30s;
    server backend_3:8000 max_fails=3 fail_timeout=30s;
}

upstream frontend_servers {
    server frontend_1:7860;
    server frontend_2:7860;
}

# HTTPé‡å®šå‘åˆ°HTTPS
server {
    listen 80;
    server_name ai-bidding.example.com;
    return 301 https://$server_name$request_uri;
}

# HTTPSä¸»é…ç½®
server {
    listen 443 ssl http2;
    server_name ai-bidding.example.com;

    # SSLé…ç½®
    ssl_certificate /etc/nginx/ssl/ai-bidding.crt;
    ssl_certificate_key /etc/nginx/ssl/ai-bidding.key;
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers ECDHE-RSA-AES256-GCM-SHA512:DHE-RSA-AES256-GCM-SHA512;
    ssl_prefer_server_ciphers off;

    # å®‰å…¨å¤´
    add_header X-Frame-Options DENY;
    add_header X-Content-Type-Options nosniff;
    add_header X-XSS-Protection "1; mode=block";
    add_header Strict-Transport-Security "max-age=63072000; includeSubDomains; preload";

    # APIä»£ç†
    location /api/ {
        proxy_pass http://backend_servers;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # è¶…æ—¶é…ç½®
        proxy_connect_timeout 60s;
        proxy_send_timeout 60s;
        proxy_read_timeout 300s;
        
        # ç¼“å†²é…ç½®
        proxy_buffering on;
        proxy_buffer_size 4k;
        proxy_buffers 8 4k;
    }

    # å‰ç«¯ä»£ç†
    location / {
        proxy_pass http://frontend_servers;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }

    # é™æ€æ–‡ä»¶æœåŠ¡
    location /uploads/ {
        alias /var/www/shared/uploads/;
        expires 1d;
        add_header Cache-Control "public, immutable";
    }

    location /outputs/ {
        alias /var/www/shared/outputs/;
        expires 1h;
        add_header Cache-Control "public";
    }

    # å¥åº·æ£€æŸ¥
    location /health {
        proxy_pass http://backend_servers/health;
        access_log off;
    }

    # ç›‘æ§ç«¯ç‚¹
    location /metrics {
        proxy_pass http://backend_servers/metrics;
        allow 10.0.0.0/8;
        allow 172.16.0.0/12;
        allow 192.168.0.0/16;
        deny all;
    }
}
```

### 3.4 æ•°æ®åº“ä¼˜åŒ–é…ç½®

#### 3.4.1 PostgreSQLé…ç½®
```ini
# postgresql.conf
# è¿æ¥é…ç½®
max_connections = 200
shared_buffers = 4GB
effective_cache_size = 12GB
work_mem = 64MB
maintenance_work_mem = 512MB

# WALé…ç½®
wal_buffers = 16MB
checkpoint_completion_target = 0.9
wal_writer_delay = 200ms

# æŸ¥è¯¢ä¼˜åŒ–
random_page_cost = 1.1
effective_io_concurrency = 200

# æ—¥å¿—é…ç½®
log_destination = 'stderr'
logging_collector = on
log_directory = 'log'
log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log'
log_min_duration_statement = 1000
log_checkpoints = on
log_connections = on
log_disconnections = on
log_lock_waits = on

# ç›‘æ§é…ç½®
shared_preload_libraries = 'pg_stat_statements'
track_activities = on
track_counts = on
track_io_timing = on
track_functions = all
```

#### 3.4.2 Redisé…ç½®
```ini
# redis.conf
# å†…å­˜é…ç½®
maxmemory 2gb
maxmemory-policy allkeys-lru

# æŒä¹…åŒ–é…ç½®
save 900 1
save 300 10
save 60 10000
appendonly yes
appendfsync everysec

# ç½‘ç»œé…ç½®
tcp-keepalive 300
timeout 0

# å®‰å…¨é…ç½®
requirepass your-redis-password
rename-command FLUSHDB ""
rename-command FLUSHALL ""
rename-command DEBUG ""

# æ—¥å¿—é…ç½®
loglevel notice
logfile /var/log/redis/redis-server.log
```

## 4. é…ç½®ç®¡ç†

### 4.1 é…ç½®æ–‡ä»¶ç»“æ„
```
config/
â”œâ”€â”€ config.toml              # ä¸»é…ç½®æ–‡ä»¶
â”œâ”€â”€ environments/            # ç¯å¢ƒç‰¹å®šé…ç½®
â”‚   â”œâ”€â”€ development.toml     # å¼€å‘ç¯å¢ƒ
â”‚   â”œâ”€â”€ testing.toml         # æµ‹è¯•ç¯å¢ƒ
â”‚   â””â”€â”€ production.toml      # ç”Ÿäº§ç¯å¢ƒ
â”œâ”€â”€ secrets/                 # æ•æ„Ÿä¿¡æ¯é…ç½®
â”‚   â”œâ”€â”€ api-keys.toml        # APIå¯†é’¥
â”‚   â””â”€â”€ database.toml        # æ•°æ®åº“å¯†ç 
â””â”€â”€ templates/               # é…ç½®æ¨¡æ¿
    â””â”€â”€ config.toml.example  # é…ç½®ç¤ºä¾‹
```

### 4.2 é…ç½®ç®¡ç†æœ€ä½³å®è·µ

#### 4.2.1 ç¯å¢ƒå˜é‡ä¼˜å…ˆçº§
1. ç¯å¢ƒå˜é‡
2. å‘½ä»¤è¡Œå‚æ•°
3. é…ç½®æ–‡ä»¶
4. é»˜è®¤å€¼

#### 4.2.2 æ•æ„Ÿä¿¡æ¯ç®¡ç†
```bash
# ä½¿ç”¨ç¯å¢ƒå˜é‡å­˜å‚¨æ•æ„Ÿä¿¡æ¯
export DATABASE_PASSWORD="secure_password"
export JWT_SECRET="super_secret_key"
export LLM_API_KEY="your_api_key"

# æˆ–ä½¿ç”¨å¯†é’¥ç®¡ç†æœåŠ¡
# AWS Secrets Manager
# Azure Key Vault
# HashiCorp Vault
```

### 4.3 é…ç½®éªŒè¯
```python
# backend/core/config_validator.py
from pydantic import BaseSettings, validator

class Settings(BaseSettings):
    database_url: str
    redis_url: str
    jwt_secret: str
    llm_api_key: str
    
    @validator('database_url')
    def validate_database_url(cls, v):
        if not v.startswith(('postgresql://', 'postgresql+asyncpg://')):
            raise ValueError('Invalid database URL')
        return v
    
    @validator('jwt_secret')
    def validate_jwt_secret(cls, v):
        if len(v) < 32:
            raise ValueError('JWT secret must be at least 32 characters')
        return v
    
    class Config:
        env_file = '.env'
        case_sensitive = False
```

## 5. ç›‘æ§å‘Šè­¦

### 5.1 Prometheusç›‘æ§é…ç½®

#### 5.1.1 Prometheusé…ç½®
```yaml
# prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

scrape_configs:
  - job_name: 'ai-bidding-backend'
    static_configs:
      - targets: ['backend:8000']
    metrics_path: '/metrics'
    scrape_interval: 30s

  - job_name: 'ai-bidding-celery'
    static_configs:
      - targets: ['flower:5555']
    metrics_path: '/metrics'
    scrape_interval: 30s

  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres-exporter:9187']

  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']

  - job_name: 'nginx'
    static_configs:
      - targets: ['nginx-exporter:9113']
```

#### 5.1.2 å‘Šè­¦è§„åˆ™
```yaml
# alert_rules.yml
groups:
  - name: ai-bidding-alerts
    rules:
      # æœåŠ¡å¯ç”¨æ€§å‘Šè­¦
      - alert: ServiceDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Service {{ $labels.instance }} is down"
          description: "{{ $labels.instance }} has been down for more than 1 minute."

      # APIå“åº”æ—¶é—´å‘Šè­¦
      - alert: HighAPILatency
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 5
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High API latency detected"
          description: "95th percentile latency is {{ $value }}s"

      # ä»»åŠ¡é˜Ÿåˆ—ç§¯å‹å‘Šè­¦
      - alert: HighTaskQueueLength
        expr: celery_queue_length > 100
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High task queue length"
          description: "Task queue length is {{ $value }}"

      # æ•°æ®åº“è¿æ¥å‘Šè­¦
      - alert: DatabaseConnectionHigh
        expr: pg_stat_activity_count > 150
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High database connections"
          description: "Database connections: {{ $value }}"

      # å†…å­˜ä½¿ç”¨å‘Šè­¦
      - alert: HighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage"
          description: "Memory usage is {{ $value }}%"

      # ç£ç›˜ç©ºé—´å‘Šè­¦
      - alert: DiskSpaceLow
        expr: (1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) * 100 > 85
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Disk space low"
          description: "Disk usage is {{ $value }}%"
```

### 5.2 Grafanaä»ªè¡¨æ¿

#### 5.2.1 ç³»ç»Ÿæ¦‚è§ˆä»ªè¡¨æ¿
- **æœåŠ¡çŠ¶æ€**ï¼šå„æœåŠ¡è¿è¡ŒçŠ¶æ€
- **è¯·æ±‚é‡**ï¼šAPIè¯·æ±‚é‡å’Œå“åº”æ—¶é—´
- **ä»»åŠ¡çŠ¶æ€**ï¼šCeleryä»»åŠ¡æ‰§è¡ŒçŠ¶æ€
- **èµ„æºä½¿ç”¨**ï¼šCPUã€å†…å­˜ã€ç£ç›˜ä½¿ç”¨ç‡
- **é”™è¯¯ç‡**ï¼šç³»ç»Ÿé”™è¯¯ç‡å’Œå¼‚å¸¸ç»Ÿè®¡

#### 5.2.2 ä¸šåŠ¡ç›‘æ§ä»ªè¡¨æ¿
- **é¡¹ç›®ç»Ÿè®¡**ï¼šé¡¹ç›®åˆ›å»ºå’Œå®Œæˆæ•°é‡
- **æ–‡æ¡£å¤„ç†**ï¼šæ–‡æ¡£ä¸Šä¼ å’Œè§£æç»Ÿè®¡
- **ç”Ÿæˆæ•ˆç‡**ï¼šæ–¹æ¡ˆç”Ÿæˆæ—¶é—´å’ŒæˆåŠŸç‡
- **ç”¨æˆ·æ´»è·ƒåº¦**ï¼šç”¨æˆ·è®¿é—®å’Œä½¿ç”¨ç»Ÿè®¡

### 5.3 æ—¥å¿—ç›‘æ§

#### 5.3.1 ELK Stacké…ç½®
```yaml
# docker-compose.monitoring.yml
version: '3.8'

services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.5.0
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms2g -Xmx2g"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data

  logstash:
    image: docker.elastic.co/logstash/logstash:8.5.0
    volumes:
      - ./logstash.conf:/usr/share/logstash/pipeline/logstash.conf
    depends_on:
      - elasticsearch

  kibana:
    image: docker.elastic.co/kibana/kibana:8.5.0
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch

volumes:
  elasticsearch_data:
```

## 6. æ—¥å¿—ç®¡ç†

### 6.1 æ—¥å¿—é…ç½®

#### 6.1.1 åº”ç”¨æ—¥å¿—é…ç½®
```python
# backend/core/logging_config.py
import logging
import sys
from pathlib import Path

def setup_logging(log_level: str = "INFO", log_format: str = "text"):
    """é…ç½®åº”ç”¨æ—¥å¿—"""

    # åˆ›å»ºæ—¥å¿—ç›®å½•
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)

    # æ—¥å¿—æ ¼å¼
    if log_format == "json":
        formatter = logging.Formatter(
            '{"timestamp": "%(asctime)s", "level": "%(levelname)s", '
            '"module": "%(name)s", "message": "%(message)s"}'
        )
    else:
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )

    # æ ¹æ—¥å¿—å™¨é…ç½®
    root_logger = logging.getLogger()
    root_logger.setLevel(getattr(logging, log_level.upper()))

    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)
    root_logger.addHandler(console_handler)

    # æ–‡ä»¶å¤„ç†å™¨
    file_handler = logging.FileHandler(log_dir / "app.log")
    file_handler.setFormatter(formatter)
    root_logger.addHandler(file_handler)

    # é”™è¯¯æ—¥å¿—å¤„ç†å™¨
    error_handler = logging.FileHandler(log_dir / "error.log")
    error_handler.setLevel(logging.ERROR)
    error_handler.setFormatter(formatter)
    root_logger.addHandler(error_handler)
```

#### 6.1.2 Logstashé…ç½®
```ruby
# logstash.conf
input {
  beats {
    port => 5044
  }

  file {
    path => "/var/log/ai-bidding/*.log"
    start_position => "beginning"
    codec => "json"
  }
}

filter {
  if [fields][service] == "ai-bidding" {
    json {
      source => "message"
    }

    date {
      match => [ "timestamp", "ISO8601" ]
    }

    mutate {
      add_field => { "service" => "ai-bidding" }
    }
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "ai-bidding-%{+YYYY.MM.dd}"
  }

  stdout {
    codec => rubydebug
  }
}
```

### 6.2 æ—¥å¿—è½®è½¬é…ç½®

#### 6.2.1 Logrotateé…ç½®
```bash
# /etc/logrotate.d/ai-bidding
/var/log/ai-bidding/*.log {
    daily
    missingok
    rotate 30
    compress
    delaycompress
    notifempty
    create 644 ai-bidding ai-bidding
    postrotate
        systemctl reload ai-bidding
    endscript
}
```

### 6.3 æ—¥å¿—åˆ†æ

#### 6.3.1 å¸¸ç”¨æŸ¥è¯¢
```bash
# æŸ¥çœ‹é”™è¯¯æ—¥å¿—
grep "ERROR" /var/log/ai-bidding/app.log | tail -100

# æŸ¥çœ‹APIè®¿é—®æ—¥å¿—
grep "api" /var/log/ai-bidding/app.log | grep "POST\|GET\|PUT\|DELETE"

# æŸ¥çœ‹ä»»åŠ¡æ‰§è¡Œæ—¥å¿—
grep "task" /var/log/ai-bidding/app.log | grep "started\|completed\|failed"

# æŸ¥çœ‹æ€§èƒ½æ—¥å¿—
grep "duration" /var/log/ai-bidding/app.log | awk '{print $NF}' | sort -n
```

## 7. å¤‡ä»½æ¢å¤

### 7.1 æ•°æ®å¤‡ä»½ç­–ç•¥

#### 7.1.1 æ•°æ®åº“å¤‡ä»½
```bash
#!/bin/bash
# backup_database.sh

# é…ç½®
DB_HOST="localhost"
DB_PORT="5432"
DB_NAME="ai_bidding"
DB_USER="ai_bidding"
BACKUP_DIR="/backup/database"
RETENTION_DAYS=30

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p $BACKUP_DIR

# ç”Ÿæˆå¤‡ä»½æ–‡ä»¶å
BACKUP_FILE="$BACKUP_DIR/ai_bidding_$(date +%Y%m%d_%H%M%S).sql"

# æ‰§è¡Œå¤‡ä»½
pg_dump -h $DB_HOST -p $DB_PORT -U $DB_USER -d $DB_NAME > $BACKUP_FILE

# å‹ç¼©å¤‡ä»½æ–‡ä»¶
gzip $BACKUP_FILE

# åˆ é™¤è¿‡æœŸå¤‡ä»½
find $BACKUP_DIR -name "*.sql.gz" -mtime +$RETENTION_DAYS -delete

echo "æ•°æ®åº“å¤‡ä»½å®Œæˆ: $BACKUP_FILE.gz"
```

#### 7.1.2 æ–‡ä»¶å¤‡ä»½
```bash
#!/bin/bash
# backup_files.sh

# é…ç½®
SOURCE_DIRS=("/app/uploads" "/app/outputs" "/app/config")
BACKUP_DIR="/backup/files"
RETENTION_DAYS=7

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p $BACKUP_DIR

# ç”Ÿæˆå¤‡ä»½æ–‡ä»¶å
BACKUP_FILE="$BACKUP_DIR/files_$(date +%Y%m%d_%H%M%S).tar.gz"

# æ‰§è¡Œå¤‡ä»½
tar -czf $BACKUP_FILE ${SOURCE_DIRS[@]}

# åˆ é™¤è¿‡æœŸå¤‡ä»½
find $BACKUP_DIR -name "*.tar.gz" -mtime +$RETENTION_DAYS -delete

echo "æ–‡ä»¶å¤‡ä»½å®Œæˆ: $BACKUP_FILE"
```

### 7.2 è‡ªåŠ¨å¤‡ä»½é…ç½®

#### 7.2.1 Crontabé…ç½®
```bash
# ç¼–è¾‘crontab
crontab -e

# æ·»åŠ å¤‡ä»½ä»»åŠ¡
# æ¯å¤©å‡Œæ™¨2ç‚¹æ‰§è¡Œæ•°æ®åº“å¤‡ä»½
0 2 * * * /opt/ai-bidding/scripts/backup_database.sh

# æ¯å¤©å‡Œæ™¨3ç‚¹æ‰§è¡Œæ–‡ä»¶å¤‡ä»½
0 3 * * * /opt/ai-bidding/scripts/backup_files.sh

# æ¯å‘¨æ—¥å‡Œæ™¨4ç‚¹æ‰§è¡Œå®Œæ•´å¤‡ä»½
0 4 * * 0 /opt/ai-bidding/scripts/backup_full.sh
```

### 7.3 æ•°æ®æ¢å¤

#### 7.3.1 æ•°æ®åº“æ¢å¤
```bash
#!/bin/bash
# restore_database.sh

# å‚æ•°æ£€æŸ¥
if [ $# -ne 1 ]; then
    echo "ç”¨æ³•: $0 <backup_file>"
    exit 1
fi

BACKUP_FILE=$1
DB_HOST="localhost"
DB_PORT="5432"
DB_NAME="ai_bidding"
DB_USER="ai_bidding"

# æ£€æŸ¥å¤‡ä»½æ–‡ä»¶
if [ ! -f "$BACKUP_FILE" ]; then
    echo "å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: $BACKUP_FILE"
    exit 1
fi

# åœæ­¢åº”ç”¨æœåŠ¡
echo "åœæ­¢åº”ç”¨æœåŠ¡..."
docker-compose stop backend celery-worker

# è§£å‹å¤‡ä»½æ–‡ä»¶ï¼ˆå¦‚æœæ˜¯å‹ç¼©çš„ï¼‰
if [[ $BACKUP_FILE == *.gz ]]; then
    gunzip -c $BACKUP_FILE > /tmp/restore.sql
    RESTORE_FILE="/tmp/restore.sql"
else
    RESTORE_FILE=$BACKUP_FILE
fi

# åˆ é™¤ç°æœ‰æ•°æ®åº“
echo "åˆ é™¤ç°æœ‰æ•°æ®åº“..."
dropdb -h $DB_HOST -p $DB_PORT -U $DB_USER $DB_NAME

# åˆ›å»ºæ–°æ•°æ®åº“
echo "åˆ›å»ºæ–°æ•°æ®åº“..."
createdb -h $DB_HOST -p $DB_PORT -U $DB_USER $DB_NAME

# æ¢å¤æ•°æ®
echo "æ¢å¤æ•°æ®..."
psql -h $DB_HOST -p $DB_PORT -U $DB_USER -d $DB_NAME < $RESTORE_FILE

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
if [ "$RESTORE_FILE" = "/tmp/restore.sql" ]; then
    rm /tmp/restore.sql
fi

# å¯åŠ¨åº”ç”¨æœåŠ¡
echo "å¯åŠ¨åº”ç”¨æœåŠ¡..."
docker-compose start backend celery-worker

echo "æ•°æ®åº“æ¢å¤å®Œæˆ"
```

## 8. æ•…éšœæ’é™¤

### 8.1 å¸¸è§é—®é¢˜è¯Šæ–­

#### 8.1.1 æœåŠ¡å¯åŠ¨å¤±è´¥
```bash
# æ£€æŸ¥æœåŠ¡çŠ¶æ€
docker-compose ps

# æŸ¥çœ‹æœåŠ¡æ—¥å¿—
docker-compose logs backend
docker-compose logs celery-worker
docker-compose logs postgres
docker-compose logs redis

# æ£€æŸ¥ç«¯å£å ç”¨
netstat -tlnp | grep :8000
netstat -tlnp | grep :5432
netstat -tlnp | grep :6379

# æ£€æŸ¥ç£ç›˜ç©ºé—´
df -h

# æ£€æŸ¥å†…å­˜ä½¿ç”¨
free -h
```

#### 8.1.2 æ•°æ®åº“è¿æ¥é—®é¢˜
```bash
# æµ‹è¯•æ•°æ®åº“è¿æ¥
psql -h localhost -p 5432 -U ai_bidding -d ai_bidding

# æ£€æŸ¥æ•°æ®åº“çŠ¶æ€
docker exec ai-bidding-postgres pg_isready -U ai_bidding

# æŸ¥çœ‹æ•°æ®åº“æ—¥å¿—
docker logs ai-bidding-postgres

# æ£€æŸ¥è¿æ¥æ•°
psql -h localhost -p 5432 -U ai_bidding -d ai_bidding -c "SELECT count(*) FROM pg_stat_activity;"
```

#### 8.1.3 Redisè¿æ¥é—®é¢˜
```bash
# æµ‹è¯•Redisè¿æ¥
redis-cli -h localhost -p 6379 ping

# æ£€æŸ¥RedisçŠ¶æ€
docker exec ai-bidding-redis redis-cli ping

# æŸ¥çœ‹Redisæ—¥å¿—
docker logs ai-bidding-redis

# æ£€æŸ¥Rediså†…å­˜ä½¿ç”¨
redis-cli -h localhost -p 6379 info memory
```

#### 8.1.4 Celeryä»»åŠ¡é—®é¢˜
```bash
# æ£€æŸ¥Celery WorkerçŠ¶æ€
docker exec ai-bidding-celery-worker celery -A backend.tasks.celery_app inspect ping

# æŸ¥çœ‹æ´»è·ƒä»»åŠ¡
docker exec ai-bidding-celery-worker celery -A backend.tasks.celery_app inspect active

# æŸ¥çœ‹ä»»åŠ¡é˜Ÿåˆ—
docker exec ai-bidding-celery-worker celery -A backend.tasks.celery_app inspect reserved

# æ¸…ç©ºä»»åŠ¡é˜Ÿåˆ—
docker exec ai-bidding-celery-worker celery -A backend.tasks.celery_app purge
```

### 8.2 æ€§èƒ½é—®é¢˜æ’æŸ¥

#### 8.2.1 APIå“åº”æ…¢
```bash
# æ£€æŸ¥APIå“åº”æ—¶é—´
curl -w "@curl-format.txt" -o /dev/null -s "http://localhost:8000/health"

# curl-format.txtå†…å®¹:
#     time_namelookup:  %{time_namelookup}\n
#        time_connect:  %{time_connect}\n
#     time_appconnect:  %{time_appconnect}\n
#    time_pretransfer:  %{time_pretransfer}\n
#       time_redirect:  %{time_redirect}\n
#  time_starttransfer:  %{time_starttransfer}\n
#                     ----------\n
#          time_total:  %{time_total}\n

# æ£€æŸ¥æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½
psql -h localhost -p 5432 -U ai_bidding -d ai_bidding -c "SELECT query, mean_time, calls FROM pg_stat_statements ORDER BY mean_time DESC LIMIT 10;"

# æ£€æŸ¥ç³»ç»Ÿè´Ÿè½½
top
htop
iostat -x 1
```

#### 8.2.2 å†…å­˜æ³„æ¼æ’æŸ¥
```bash
# æ£€æŸ¥å®¹å™¨å†…å­˜ä½¿ç”¨
docker stats

# æ£€æŸ¥Pythonè¿›ç¨‹å†…å­˜
ps aux | grep python | awk '{print $2, $4, $11}' | sort -k2 -nr

# ä½¿ç”¨å†…å­˜åˆ†æå·¥å…·
pip install memory-profiler
python -m memory_profiler your_script.py
```

### 8.3 æ•…éšœæ¢å¤æµç¨‹

#### 8.3.1 æœåŠ¡æ•…éšœæ¢å¤
```bash
#!/bin/bash
# service_recovery.sh

echo "å¼€å§‹æœåŠ¡æ•…éšœæ¢å¤..."

# 1. åœæ­¢æ‰€æœ‰æœåŠ¡
echo "åœæ­¢æ‰€æœ‰æœåŠ¡..."
docker-compose down

# 2. æ¸…ç†å®¹å™¨å’Œç½‘ç»œ
echo "æ¸…ç†å®¹å™¨å’Œç½‘ç»œ..."
docker system prune -f

# 3. æ£€æŸ¥ç£ç›˜ç©ºé—´
echo "æ£€æŸ¥ç£ç›˜ç©ºé—´..."
df -h

# 4. é‡æ–°å¯åŠ¨æœåŠ¡
echo "é‡æ–°å¯åŠ¨æœåŠ¡..."
docker-compose up -d

# 5. ç­‰å¾…æœåŠ¡å¯åŠ¨
echo "ç­‰å¾…æœåŠ¡å¯åŠ¨..."
sleep 30

# 6. å¥åº·æ£€æŸ¥
echo "æ‰§è¡Œå¥åº·æ£€æŸ¥..."
curl -f http://localhost:8000/health || exit 1

echo "æœåŠ¡æ¢å¤å®Œæˆ"
```

#### 8.3.2 æ•°æ®æ•…éšœæ¢å¤
```bash
#!/bin/bash
# data_recovery.sh

echo "å¼€å§‹æ•°æ®æ•…éšœæ¢å¤..."

# 1. åœæ­¢åº”ç”¨æœåŠ¡
echo "åœæ­¢åº”ç”¨æœåŠ¡..."
docker-compose stop backend celery-worker

# 2. æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
echo "æ£€æŸ¥æ•°æ®å®Œæ•´æ€§..."
docker exec ai-bidding-postgres pg_dump --schema-only ai_bidding > /tmp/schema_check.sql

# 3. ä»æœ€æ–°å¤‡ä»½æ¢å¤
echo "ä»æœ€æ–°å¤‡ä»½æ¢å¤..."
LATEST_BACKUP=$(ls -t /backup/database/*.sql.gz | head -1)
./restore_database.sh $LATEST_BACKUP

# 4. éªŒè¯æ•°æ®æ¢å¤
echo "éªŒè¯æ•°æ®æ¢å¤..."
psql -h localhost -p 5432 -U ai_bidding -d ai_bidding -c "SELECT count(*) FROM projects;"

# 5. é‡å¯åº”ç”¨æœåŠ¡
echo "é‡å¯åº”ç”¨æœåŠ¡..."
docker-compose start backend celery-worker

echo "æ•°æ®æ¢å¤å®Œæˆ"
```

---

**æ–‡æ¡£ç‰ˆæœ¬**ï¼šv2.0
**æœ€åæ›´æ–°**ï¼š2025-07-02
**ç»´æŠ¤äººå‘˜**ï¼šAIæŠ•æ ‡ç³»ç»Ÿå¼€å‘å›¢é˜Ÿ
